{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:37.934090Z",
     "iopub.status.busy": "2024-11-13T13:04:37.933255Z",
     "iopub.status.idle": "2024-11-13T13:04:42.711555Z",
     "shell.execute_reply": "2024-11-13T13:04:42.710724Z",
     "shell.execute_reply.started": "2024-11-13T13:04:37.934043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.713488Z",
     "iopub.status.busy": "2024-11-13T13:04:42.713045Z",
     "iopub.status.idle": "2024-11-13T13:04:42.718381Z",
     "shell.execute_reply": "2024-11-13T13:04:42.717329Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.713453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk import word_tokenize \n",
    "import re\n",
    "\n",
    "def load_data(path: str) -> json:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels() -> dict:\n",
    "    return {\n",
    "        'NotMentioned': 0,\n",
    "        'Entailment': 1,\n",
    "        'Contradiction': 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(str: str) -> str:\n",
    "    str = str.replace('\\n', ' ')\n",
    "    str = re.sub(r'\\\\t', ' ', str)\n",
    "    str = re.sub(r'\\\\r', ' ', str)\n",
    "    str = re.sub(r'(.)\\1{2,}', r'\\1', str)\n",
    "    return str.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesis(data: dict) -> list:\n",
    "    hypothesis = {}\n",
    "    for key, value in data['labels'].items():\n",
    "        hypothesis[key] = clean_str(value['hypothesis'])\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(str: str) -> str:\n",
    "    return ' '.join(word_tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.719929Z",
     "iopub.status.busy": "2024-11-13T13:04:42.719546Z",
     "iopub.status.idle": "2024-11-13T13:04:42.763489Z",
     "shell.execute_reply": "2024-11-13T13:04:42.762515Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.719895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.766733Z",
     "iopub.status.busy": "2024-11-13T13:04:42.766251Z",
     "iopub.status.idle": "2024-11-13T13:04:42.775058Z",
     "shell.execute_reply": "2024-11-13T13:04:42.774126Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.766685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"batch_size\": 32,\n",
    "    \"train_path\": \"/kaggle/input/project-data/train (1).json\",\n",
    "    \"test_path\": \"/kaggle/input/project-data/test (1).json\",\n",
    "    \"dev_path\": \"/kaggle/input/project-data/dev (1).json\",\n",
    "    \"max_length\": 512,\n",
    "    \"models_save_dir\": \"/kaggle/working/saved_model\",\n",
    "    \"results_dir\": \"/kaggle/working/results\",\n",
    "    \"dataset_dir\": \"/kaggle/working/dataset_dir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.776934Z",
     "iopub.status.busy": "2024-11-13T13:04:42.776554Z",
     "iopub.status.idle": "2024-11-13T13:04:42.785101Z",
     "shell.execute_reply": "2024-11-13T13:04:42.784307Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.776889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.786591Z",
     "iopub.status.busy": "2024-11-13T13:04:42.786288Z",
     "iopub.status.idle": "2024-11-13T13:04:44.018928Z",
     "shell.execute_reply": "2024-11-13T13:04:44.017975Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.786559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4777fcab0094906aa646ce36a66f518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d196416a418433289473755db5db40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9498fcdba93b4fa686420c1d232be064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe101b768f944debafd961280eae6fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/saved_model/tokenizer_config.json',\n",
       " '/kaggle/working/saved_model/special_tokens_map.json',\n",
       " '/kaggle/working/saved_model/vocab.txt',\n",
       " '/kaggle/working/saved_model/added_tokens.json',\n",
       " '/kaggle/working/saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "\n",
    "tokenizer.save_pretrained(cfg['models_save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:44.020734Z",
     "iopub.status.busy": "2024-11-13T13:04:44.020332Z",
     "iopub.status.idle": "2024-11-13T13:04:44.062312Z",
     "shell.execute_reply": "2024-11-13T13:04:44.061504Z",
     "shell.execute_reply.started": "2024-11-13T13:04:44.020689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['models_save_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:44.064348Z",
     "iopub.status.busy": "2024-11-13T13:04:44.063675Z",
     "iopub.status.idle": "2024-11-13T13:04:56.891407Z",
     "shell.execute_reply": "2024-11-13T13:04:56.890450Z",
     "shell.execute_reply.started": "2024-11-13T13:04:44.064300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icecream\n",
      "  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
      "Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: icecream\n",
      "Successfully installed icecream-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install icecream\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:56.893088Z",
     "iopub.status.busy": "2024-11-13T13:04:56.892763Z",
     "iopub.status.idle": "2024-11-13T13:04:56.897962Z",
     "shell.execute_reply": "2024-11-13T13:04:56.897053Z",
     "shell.execute_reply.started": "2024-11-13T13:04:56.893054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_hypothesis_idx(hypothesis_name):\n",
    "    return int(hypothesis_name.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:56.902589Z",
     "iopub.status.busy": "2024-11-13T13:04:56.902215Z",
     "iopub.status.idle": "2024-11-13T13:04:56.928795Z",
     "shell.execute_reply": "2024-11-13T13:04:56.927912Z",
     "shell.execute_reply.started": "2024-11-13T13:04:56.902554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, documents, tokenizer, hypothesis, context_sizes, surround_character_size):\n",
    "        label_dict = get_labels()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n",
    "\n",
    "        data_points = []\n",
    "        contexts = [{}]\n",
    "\n",
    "        for context_size in context_sizes:\n",
    "            for i, doc in enumerate(documents):\n",
    "                char_idx = 0\n",
    "                while char_idx < len(doc['text']):\n",
    "                    ic(char_idx)\n",
    "                    document_spans = doc['spans']\n",
    "                    cur_context = {\n",
    "                        'doc_id': i,\n",
    "                        'start_char_idx': char_idx,\n",
    "                        'end_char_idx': char_idx + context_size,\n",
    "                        'spans' : [],\n",
    "                    }\n",
    "\n",
    "                    for j, (start, end) in enumerate(document_spans):\n",
    "                        if end <= char_idx:\n",
    "                            continue\n",
    "\n",
    "                        cur_context['spans'].append({\n",
    "                            'start_char_idx': max(start, char_idx),\n",
    "                            'end_char_idx': min(end, char_idx + context_size),\n",
    "                            'marked': start >= char_idx and end <= char_idx + context_size,\n",
    "                            'span_id': j\n",
    "                        })\n",
    "\n",
    "                        if end > char_idx + context_size:\n",
    "                            break\n",
    "\n",
    "                    if cur_context == contexts[-1]:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                        continue\n",
    "\n",
    "                    contexts.append(cur_context)\n",
    "                    if len(cur_context['spans']) == 1 and not cur_context['spans'][0]['marked']:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                    else:\n",
    "                        char_idx = cur_context['spans'][-1]['start_char_idx'] - surround_character_size\n",
    "\n",
    "        contexts.pop(0)\n",
    "\n",
    "        for nda_name, nda_desc in hypothesis.items():\n",
    "            for i, context in enumerate(contexts):\n",
    "\n",
    "                nli_label = label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n",
    "\n",
    "                data_point = {}\n",
    "                data_point['hypotheis'] = nda_desc\n",
    "                cur_premise = \"\"\n",
    "                data_point['marked_beg'] = context['spans'][0]['marked']\n",
    "                data_point['marked_end'] = context['spans'][-1]['marked']\n",
    "                doc_id = context['doc_id']\n",
    "                hypothesis_id = get_hypothesis_idx(nda_name)\n",
    "                span_ids = []\n",
    "\n",
    "                if len(context['spans']) == 1:\n",
    "                    data_point['marked_end'] = True\n",
    "\n",
    "                span_labels = []\n",
    "\n",
    "                for span in context['spans']:\n",
    "                    val = int(span['span_id'] in documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['spans'])\n",
    "\n",
    "                    val = 2 * val - 1 # making 0 -> -1 and 1 -> 1\n",
    "\n",
    "                    if span['marked']:\n",
    "                        span_labels.append(val)\n",
    "                        span_ids.append(span['span_id'])\n",
    "\n",
    "                    cur_premise += ' [SPAN] '\n",
    "                    cur_premise += documents[context['doc_id']]['text'][span['start_char_idx']:span['end_char_idx']]\n",
    "\n",
    "                data_point['premise'] = cur_premise\n",
    "                \n",
    "                if nli_label == get_labels()['NotMentioned']:\n",
    "                    span_labels = torch.zeros(len(span_labels), dtype=torch.long)\n",
    "\n",
    "                data_point['nli_label'] = torch.tensor(nli_label, dtype=torch.long)\n",
    "                data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n",
    "                data_point['doc_id'] = torch.tensor(doc_id, dtype=torch.long)\n",
    "                data_point['hypothesis_id'] = torch.tensor(hypothesis_id, dtype=torch.long)\n",
    "                data_point['span_ids'] = torch.tensor(span_ids, dtype=torch.long)\n",
    "\n",
    "                data_points.append(data_point)\n",
    "\n",
    "        self.data_points = data_points\n",
    "        self.span_token_id = self.tokenizer.convert_tokens_to_ids('[SPAN]')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_data = self.tokenizer(\n",
    "            [self.data_points[idx]['hypotheis']],\n",
    "            [self.data_points[idx]['premise']],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        tokenized_data['input_ids'] = tokenized_data['input_ids'].squeeze()\n",
    "        tokenized_data['attention_mask'] = tokenized_data['attention_mask'].squeeze()\n",
    "        tokenized_data['token_type_ids'] = tokenized_data['token_type_ids'].squeeze()\n",
    "\n",
    "        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n",
    "\n",
    "        if not self.data_points[idx]['marked_beg']:\n",
    "            span_indices = span_indices[1:]\n",
    "        \n",
    "        if not self.data_points[idx]['marked_end'] or tokenized_data['attention_mask'][-1] == 0:\n",
    "            span_indices = span_indices[:-1]\n",
    "        \n",
    "        span_ids = self.data_points[idx]['span_ids']\n",
    "        span_ids = span_ids[:len(span_indices)]\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'],\n",
    "            'attention_mask': tokenized_data['attention_mask'],\n",
    "            'token_type_ids': tokenized_data['token_type_ids'],\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': self.data_points[idx]['nli_label'],\n",
    "            'span_labels': self.data_points[idx]['span_labels'][:len(span_indices)],\n",
    "            'data_for_metrics': {\n",
    "                'doc_id': self.data_points[idx]['doc_id'],\n",
    "                'hypothesis_id': self.data_points[idx]['hypothesis_id'],\n",
    "                'span_ids': span_ids,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:58.643818Z",
     "iopub.status.busy": "2024-11-13T13:04:58.643022Z",
     "iopub.status.idle": "2024-11-13T13:05:07.463063Z",
     "shell.execute_reply": "2024-11-13T13:05:07.462257Z",
     "shell.execute_reply.started": "2024-11-13T13:04:58.643771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/1745476268.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(os.path.join(cfg['train_path']))\n",
    "dev_data = load_data(os.path.join(cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(train_data)\n",
    "\n",
    "train_data = train_data['documents']\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(train_data), len(dev_data), len(test_data))\n",
    "train_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1100], 50)\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del train_data\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:05:07.464417Z",
     "iopub.status.busy": "2024-11-13T13:05:07.464119Z",
     "iopub.status.idle": "2024-11-13T13:05:07.733155Z",
     "shell.execute_reply": "2024-11-13T13:05:07.732257Z",
     "shell.execute_reply.started": "2024-11-13T13:05:07.464386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(train_dataset): 97546\n",
      "    len(dev_dataset): 15385\n",
      "    len(test_dataset): 28645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97546, 15385, 28645)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(len(train_dataset), len(dev_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:05:07.734676Z",
     "iopub.status.busy": "2024-11-13T13:05:07.734379Z",
     "iopub.status.idle": "2024-11-13T13:05:07.741927Z",
     "shell.execute_reply": "2024-11-13T13:05:07.740927Z",
     "shell.execute_reply.started": "2024-11-13T13:05:07.734639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "def get_class_weights(dataset):\n",
    "    nli_labels = [x['nli_label'] for x in dataset]\n",
    "\n",
    "    span_labels = []\n",
    "    for x in dataset:\n",
    "        span_labels.extend(x['span_labels'].tolist())\n",
    "\n",
    "    nli_weights = compute_class_weight('balanced', classes=np.unique(nli_labels), y=np.array(nli_labels))\n",
    "\n",
    "    nli_weights = nli_weights.tolist()\n",
    "\n",
    "    span_labels = [x for x in span_labels if x != -1]\n",
    "    span_labels = np.array(span_labels)\n",
    "    span_weight = np.sum(span_labels == 0) / np.sum(span_labels)\n",
    "\n",
    "    return nli_weights, span_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:05:07.743369Z",
     "iopub.status.busy": "2024-11-13T13:05:07.743057Z",
     "iopub.status.idle": "2024-11-13T13:10:33.657826Z",
     "shell.execute_reply": "2024-11-13T13:10:33.657000Z",
     "shell.execute_reply.started": "2024-11-13T13:05:07.743337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nli_weights, span_weight = get_class_weights(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:33.659279Z",
     "iopub.status.busy": "2024-11-13T13:10:33.658958Z",
     "iopub.status.idle": "2024-11-13T13:10:33.707815Z",
     "shell.execute_reply": "2024-11-13T13:10:33.706904Z",
     "shell.execute_reply.started": "2024-11-13T13:10:33.659245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| nli_weights: [0.9712447975785092, 0.6134852801519468, 2.9380440348182284]\n",
      "    span_weight: 24.93889485618809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9712447975785092, 0.6134852801519468, 2.9380440348182284],\n",
       " 24.93889485618809)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(nli_weights, span_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:33.709739Z",
     "iopub.status.busy": "2024-11-13T13:10:33.709254Z",
     "iopub.status.idle": "2024-11-13T13:10:34.487045Z",
     "shell.execute_reply": "2024-11-13T13:10:34.486254Z",
     "shell.execute_reply.started": "2024-11-13T13:10:33.709685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class ContractNLIConfig(PretrainedConfig):\n",
    "    # def __init__(self, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(get_labels()), ignore_span_label = 2, nli_weights = nli_weights, span_weight = span_weight, **kwargs):\n",
    "    def __init__(self, nli_weights = [1, 1, 1], span_weight = 1, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(get_labels()), ignore_span_label = 2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.lambda_ = lambda_\n",
    "        self.ignore_span_label = ignore_span_label\n",
    "        self.nli_weights = nli_weights\n",
    "        self.span_weight = span_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:34.488739Z",
     "iopub.status.busy": "2024-11-13T13:10:34.488287Z",
     "iopub.status.idle": "2024-11-13T13:10:34.506379Z",
     "shell.execute_reply": "2024-11-13T13:10:34.505338Z",
     "shell.execute_reply.started": "2024-11-13T13:10:34.488699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "class ContractNLI(PreTrainedModel):\n",
    "    config_class = ContractNLIConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n",
    "        self.bert.resize_token_embeddings(self.bert.config.vocab_size + 1, pad_to_multiple_of=8)\n",
    "        self.bert.eval()\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding_dim = self.bert.config.hidden_size\n",
    "        self.num_labels = config.num_labels\n",
    "        self.lambda_ = config.lambda_\n",
    "        self.nli_criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.config.nli_weights, dtype=torch.float32))\n",
    "        self.span_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.config.span_weight, dtype=torch.float32))\n",
    "\n",
    "        self.span_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 2, 1)\n",
    "        )\n",
    "\n",
    "        self.nli_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 2, self.num_labels)\n",
    "        )\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # use the same initialization as bert\n",
    "            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, span_indices):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True).hidden_states[-4:]\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        outputs = outputs.permute([1, 2, 0, 3])\n",
    "        outputs = outputs.reshape([outputs.shape[0], outputs.shape[1], -1])\n",
    "\n",
    "        gather = torch.gather(outputs, 1, span_indices.unsqueeze(2).expand(-1, -1, outputs.shape[-1]))\n",
    "\n",
    "        masked_gather = gather[span_indices != 0]\n",
    "        span_logits = self.span_classifier(masked_gather)\n",
    "        nli_logits = self.nli_classifier(outputs[:, 0, :])\n",
    "\n",
    "        return span_logits, nli_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:34.508453Z",
     "iopub.status.busy": "2024-11-13T13:10:34.507898Z",
     "iopub.status.idle": "2024-11-13T13:10:48.833347Z",
     "shell.execute_reply": "2024-11-13T13:10:48.832532Z",
     "shell.execute_reply.started": "2024-11-13T13:10:34.508405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class ContractNLITrainer(Trainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        span_label = inputs.pop('span_labels')\n",
    "        nli_label = inputs.pop('nli_label')\n",
    "        inputs.pop('data_for_metrics')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        span_logits, nli_logits = outputs[0], outputs[1]\n",
    "        \n",
    "        # span labels = -1, means ignore \n",
    "        \n",
    "        mask = span_label != -1\n",
    "        span_label = span_label[mask]\n",
    "        span_logits = span_logits[mask]\n",
    "        \n",
    "        span_label = span_label.float()\n",
    "        span_logits = span_logits.float()\n",
    "        \n",
    "        span_label = span_label.view(-1)\n",
    "        span_logits = span_logits.view(-1)        \n",
    "\n",
    "        # if len(true_span_labels) == 0 or len(pred_span_labels) != len(true_span_labels):\n",
    "        #     span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "        # else:\n",
    "        #     span_loss = self.model.span_criterion(pred_span_labels, true_span_labels)\n",
    "        \n",
    "        if len(span_label) == 0:\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "        else:\n",
    "            span_loss = self.model.span_criterion(span_logits, span_label)\n",
    "\n",
    "        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n",
    "\n",
    "        if torch.isnan(nli_loss):\n",
    "            nli_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        if torch.isnan(span_loss):\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        loss = span_loss + self.model.lambda_ * nli_loss\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            loss = torch.tensor(0, dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(features):\n",
    "        span_indices_list = [feature['span_indices'] for feature in features]\n",
    "        max_len = max([len(span_indices) for span_indices in span_indices_list])\n",
    "        span_indices_list = [torch.cat([span_indices, torch.zeros(max_len - len(span_indices), dtype=torch.long)]) for span_indices in span_indices_list]\n",
    "\n",
    "        span_ids_list = [feature['data_for_metrics']['span_ids'] for feature in features]\n",
    "        max_len = max([len(span_ids) for span_ids in span_ids_list])\n",
    "        \n",
    "        # pad to get the doc id and hypothesis id for each input while evaluating\n",
    "        span_ids_list = [torch.cat([span_ids, torch.full((max_len - len(span_ids),), -1)]) for span_ids in span_ids_list]\n",
    "        \n",
    "        input_ids = torch.stack([feature['input_ids'] for feature in features])\n",
    "        attention_mask = torch.stack([feature['attention_mask'] for feature in features])\n",
    "        token_type_ids = torch.stack([feature['token_type_ids'] for feature in features])\n",
    "        span_indices = torch.stack(span_indices_list)\n",
    "        nli_label = torch.stack([feature['nli_label'] for feature in features])\n",
    "        span_label = torch.cat([feature['span_labels'] for feature in features], dim=0)\n",
    "        data_for_metrics = {\n",
    "            'doc_id': torch.stack([feature['data_for_metrics']['doc_id'] for feature in features]),\n",
    "            'hypothesis_id': torch.stack([feature['data_for_metrics']['hypothesis_id'] for feature in features]),\n",
    "            'span_ids': torch.stack(span_ids_list),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': nli_label,\n",
    "            'span_labels': span_label,\n",
    "            'data_for_metrics': data_for_metrics,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.835005Z",
     "iopub.status.busy": "2024-11-13T13:10:48.834427Z",
     "iopub.status.idle": "2024-11-13T13:10:48.895472Z",
     "shell.execute_reply": "2024-11-13T13:10:48.894416Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.834972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],   \n",
    "    num_train_epochs=10,           \n",
    "    gradient_accumulation_steps=4,   \n",
    "    logging_strategy='epoch',\n",
    "    eval_steps=2,\n",
    "    save_steps=2,\n",
    "    logging_steps=2,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.896893Z",
     "iopub.status.busy": "2024-11-13T13:10:48.896588Z",
     "iopub.status.idle": "2024-11-13T13:10:48.902167Z",
     "shell.execute_reply": "2024-11-13T13:10:48.901279Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.896860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wandb_hp_space(trial):\n",
    "    return {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\n",
    "            \"name\": \"eval/loss\",\n",
    "            \"goal\": \"minimize\"\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\n",
    "                \"values\": [1e-5, 3e-5, 5e-5]\n",
    "            },\n",
    "            \"lambda_\": {\n",
    "                \"values\": [0.05, 0.1, 0.4]\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.908677Z",
     "iopub.status.busy": "2024-11-13T13:10:48.908371Z",
     "iopub.status.idle": "2024-11-13T13:10:48.914556Z",
     "shell.execute_reply": "2024-11-13T13:10:48.913652Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.908644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    if trial is None:\n",
    "        return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight))\n",
    "\n",
    "    return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight, lambda_=trial['lambda_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.915819Z",
     "iopub.status.busy": "2024-11-13T13:10:48.915540Z",
     "iopub.status.idle": "2024-11-13T13:10:52.727740Z",
     "shell.execute_reply": "2024-11-13T13:10:52.726696Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.915787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f03a6e54aa40739f71604b6e04fb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer = ContractNLITrainer(\n",
    "    model=None,                       \n",
    "    args=training_args,              \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=dev_dataset,     \n",
    "    data_collator=ContractNLITrainer.collate_fn,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n",
    "    model_init=model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T09:43:24.421175Z",
     "iopub.status.busy": "2024-11-11T09:43:24.420760Z",
     "iopub.status.idle": "2024-11-11T11:41:08.348319Z",
     "shell.execute_reply": "2024-11-11T11:41:08.347406Z",
     "shell.execute_reply.started": "2024-11-11T09:43:24.421128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7456' max='12420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7456/12420 1:57:38 < 1:18:20, 1.06 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.634800</td>\n",
       "      <td>3.922571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.283600</td>\n",
       "      <td>4.243315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.186900</td>\n",
       "      <td>2.967618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.090300</td>\n",
       "      <td>5.793064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>5.055171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7456, training_loss=2.2392635099877616, metrics={'train_runtime': 7060.2097, 'train_samples_per_second': 56.32, 'train_steps_per_second': 1.759, 'total_flos': 8.353250792946893e+16, 'train_loss': 2.2392635099877616, 'epoch': 5.9995976664655})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:23.766598Z",
     "iopub.status.busy": "2024-11-13T13:11:23.765748Z",
     "iopub.status.idle": "2024-11-13T13:11:23.771173Z",
     "shell.execute_reply": "2024-11-13T13:11:23.770140Z",
     "shell.execute_reply.started": "2024-11-13T13:11:23.766557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:23.989070Z",
     "iopub.status.busy": "2024-11-13T13:11:23.988365Z",
     "iopub.status.idle": "2024-11-13T13:11:23.993842Z",
     "shell.execute_reply": "2024-11-13T13:11:23.992931Z",
     "shell.execute_reply.started": "2024-11-13T13:11:23.989030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['WANDB_ENTITY'] = 'contract-nli-db'\n",
    "os.environ['WANDB_PROJECT'] = 'contract-nli-metric'\n",
    "os.environ['WANDB_LOG_MODEL'] = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.248340Z",
     "iopub.status.busy": "2024-11-13T13:11:24.247665Z",
     "iopub.status.idle": "2024-11-13T13:11:24.254725Z",
     "shell.execute_reply": "2024-11-13T13:11:24.253762Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.248300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.397342Z",
     "iopub.status.busy": "2024-11-13T13:11:24.396572Z",
     "iopub.status.idle": "2024-11-13T13:11:24.405080Z",
     "shell.execute_reply": "2024-11-13T13:11:24.404122Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.397302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_path': '/kaggle/input/project-data/train (1).json',\n",
       " 'test_path': '/kaggle/input/project-data/test (1).json',\n",
       " 'dev_path': '/kaggle/input/project-data/dev (1).json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': '/kaggle/input/anlp-project-trained-model/checkpoint',\n",
       " 'dataset_dir': './scratch/shu7bh/contract_nli/dataset',\n",
       " 'results_dir': './scratch/shu7bh/contract_nli/results',\n",
       " 'trained_model_dir': '/kaggle/input/anlp-project-trained-model/',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cfg = {\n",
    "    \"train_path\": \"/kaggle/input/project-data/train (1).json\",\n",
    "    \"test_path\": \"/kaggle/input/project-data/test (1).json\",\n",
    "    \"dev_path\": \"/kaggle/input/project-data/dev (1).json\",\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"max_length\": 512,\n",
    "    \"models_save_dir\": \"/kaggle/input/anlp-project-trained-model/checkpoint\",\n",
    "    \"dataset_dir\": \"./scratch/shu7bh/contract_nli/dataset\",\n",
    "    \"results_dir\": \"./scratch/shu7bh/contract_nli/results\",\n",
    "    \"trained_model_dir\": \"/kaggle/input/anlp-project-trained-model/\",\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.589322Z",
     "iopub.status.busy": "2024-11-13T13:11:24.588588Z",
     "iopub.status.idle": "2024-11-13T13:11:24.603685Z",
     "shell.execute_reply": "2024-11-13T13:11:24.602745Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.589281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.820176Z",
     "iopub.status.busy": "2024-11-13T13:11:24.819300Z",
     "iopub.status.idle": "2024-11-13T13:11:24.987468Z",
     "shell.execute_reply": "2024-11-13T13:11:24.986482Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.820132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:25.040608Z",
     "iopub.status.busy": "2024-11-13T13:11:25.039896Z",
     "iopub.status.idle": "2024-11-13T13:11:36.679615Z",
     "shell.execute_reply": "2024-11-13T13:11:36.678400Z",
     "shell.execute_reply.started": "2024-11-13T13:11:25.040562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: icecream in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install icecream\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:15:59.572050Z",
     "iopub.status.busy": "2024-11-13T13:15:59.571256Z",
     "iopub.status.idle": "2024-11-13T13:16:01.973821Z",
     "shell.execute_reply": "2024-11-13T13:16:01.972781Z",
     "shell.execute_reply.started": "2024-11-13T13:15:59.572010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/1745476268.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "dev_data = load_data(os.path.join(cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(dev_data)\n",
    "\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "# dev_data = dev_data[:50]\n",
    "# test_data = test_data[:50]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(dev_data), len(test_data))\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.975854Z",
     "iopub.status.busy": "2024-11-13T13:16:01.975540Z",
     "iopub.status.idle": "2024-11-13T13:16:01.980990Z",
     "shell.execute_reply": "2024-11-13T13:16:01.979941Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.975820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15385\n",
      "28645\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.982488Z",
     "iopub.status.busy": "2024-11-13T13:16:01.982137Z",
     "iopub.status.idle": "2024-11-13T13:16:01.993820Z",
     "shell.execute_reply": "2024-11-13T13:16:01.992952Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.982454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "def get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return np.interp(recall_level, recall[::-1], precision[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.996042Z",
     "iopub.status.busy": "2024-11-13T13:16:01.995718Z",
     "iopub.status.idle": "2024-11-13T13:16:02.008766Z",
     "shell.execute_reply": "2024-11-13T13:16:02.007841Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.996008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import numpy and sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def calculate_micro_average_precision(y_true, y_pred):\n",
    "\n",
    "    \"\"\"Calculate the micro average precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Micro average precision score.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    if num_classes == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = 0.0\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        y_true_indices = np.where(y_true == class_idx)\n",
    "        average_precision += ic(precision_score(\n",
    "            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
    "        ))\n",
    "\n",
    "    return average_precision / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.010166Z",
     "iopub.status.busy": "2024-11-13T13:16:02.009879Z",
     "iopub.status.idle": "2024-11-13T13:16:02.023568Z",
     "shell.execute_reply": "2024-11-13T13:16:02.022770Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.010135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score_for_class(y_true, y_pred, class_idx):\n",
    "    \"\"\"Calculate the F1 score for a given class.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        class_idx (int): Index of the class.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score for the given class.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true_indices = np.where(y_true == class_idx)\n",
    "    \n",
    "    return f1_score(\n",
    "        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.024946Z",
     "iopub.status.busy": "2024-11-13T13:16:02.024629Z",
     "iopub.status.idle": "2024-11-13T13:16:02.034045Z",
     "shell.execute_reply": "2024-11-13T13:16:02.033189Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.024908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def precision_at_recall(y_true, y_scores, recall_threshold):\n",
    "    precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "    idx = (np.abs(recall - recall_threshold)).argmin()  # Find nearest recall value to threshold\n",
    "    ic(threshold[idx])\n",
    "    return precision[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.035536Z",
     "iopub.status.busy": "2024-11-13T13:16:02.035247Z",
     "iopub.status.idle": "2024-11-13T13:16:02.069079Z",
     "shell.execute_reply": "2024-11-13T13:16:02.068183Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.035505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],  \n",
    "    num_train_epochs=10,       \n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_strategy='epoch',\n",
    "    # eval_steps=0.25,\n",
    "    # save_steps=0.25,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.070545Z",
     "iopub.status.busy": "2024-11-13T13:16:02.070167Z",
     "iopub.status.idle": "2024-11-13T13:16:02.076385Z",
     "shell.execute_reply": "2024-11-13T13:16:02.075479Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.070500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/anlp-project-trained-model/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['trained_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.078298Z",
     "iopub.status.busy": "2024-11-13T13:16:02.077666Z",
     "iopub.status.idle": "2024-11-13T13:16:03.189546Z",
     "shell.execute_reply": "2024-11-13T13:16:03.188512Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.078253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the directory where your model is stored locally.\n",
    "artifact_dir = '/kaggle/input/fully-trained-model/checkpoint-12194'  # Replace this with the actual path\n",
    "\n",
    "# Load the model directly from the local directory\n",
    "model = ContractNLI.from_pretrained(artifact_dir).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T14:31:46.799835Z",
     "iopub.status.busy": "2024-11-13T14:31:46.799461Z",
     "iopub.status.idle": "2024-11-13T14:31:46.825459Z",
     "shell.execute_reply": "2024-11-13T14:31:46.824509Z",
     "shell.execute_reply.started": "2024-11-13T14:31:46.799795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContractNLIMetricTrainer(ContractNLITrainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        self.model.eval()\n",
    "        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n",
    "\n",
    "        eval_nli_labels = []\n",
    "        eval_nli_preds = []\n",
    "        true_labels_per_span = {}\n",
    "        probs_per_span = {}\n",
    "\n",
    "        nli_metrics = {}\n",
    "\n",
    "        for inputs in tqdm(self.dataloader):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            span_labels = inputs.pop('span_labels')\n",
    "            nli_labels = inputs.pop('nli_label')\n",
    "            data_for_metrics = inputs.pop('data_for_metrics')\n",
    "\n",
    "            span_indices_to_consider = torch.where(span_labels != -1)[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "                span_labels = span_labels.float()\n",
    "                span_logits = span_logits.float()\n",
    "                \n",
    "                span_labels = span_labels.view(-1)\n",
    "                span_logits = span_logits.view(-1)\n",
    "\n",
    "                # start_index = 0\n",
    "                \n",
    "                indices_considered = 0 # total number of span indices considered\n",
    "\n",
    "                # find the corresponding span index in data_for_metrics['span_ids'] considering -1 to be padding index\n",
    "                # ic(span_index)\n",
    "                for i, span_index_row in enumerate(data_for_metrics['span_ids']):\n",
    "                    current_index = 0 # current row's first -1 index\n",
    "                    # ic(span_index_row)\n",
    "                    first_minus_one_index = torch.where(span_index_row == -1)[0]\n",
    "                    # ic(first_minus_one_index)\n",
    "                    if len(first_minus_one_index) == 0:\n",
    "                        first_minus_one_index = len(span_index_row)\n",
    "                    else:\n",
    "                        first_minus_one_index = first_minus_one_index[0].item()\n",
    "\n",
    "                    key = str(data_for_metrics['doc_id'][i].item())+ '-' + str(data_for_metrics['hypothesis_id'][i].item())\n",
    "\n",
    "                    # mask span_labels and span_logits for the current row\n",
    "                    mask = span_labels[indices_considered:indices_considered+first_minus_one_index] != -1\n",
    "                    span_logits_masked = span_logits[indices_considered:indices_considered+first_minus_one_index][mask]\n",
    "\n",
    "                    spans_contribution = torch.sum(torch.sigmoid(span_logits_masked)) / (len(span_logits_masked)) \n",
    "\n",
    "                    if key in nli_metrics:\n",
    "                        nli_metrics[key]['spans_contribution'].append(spans_contribution)\n",
    "                        nli_metrics[key]['nli_logits'].append(nli_logits[i])\n",
    "                    else:\n",
    "                        nli_metrics[key] = {}\n",
    "                        nli_metrics[key]['true_nli_labels'] = nli_labels[i]\n",
    "                        nli_metrics[key]['spans_contribution'] = [spans_contribution]\n",
    "                        nli_metrics[key]['nli_logits'] = [nli_logits[i]]\n",
    "                    \n",
    "                    current_index = first_minus_one_index\n",
    "                    indices_considered += current_index\n",
    "                    \n",
    "                    # ic(indices_considered)\n",
    "                    # ic(current_index)\n",
    "                    cnt = 0 # count to keep track of the number of span indices added in dictionary\n",
    "                    \n",
    "                    for span_index in span_indices_to_consider:\n",
    "\n",
    "                        if span_index < indices_considered:\n",
    "                            cnt += 1\n",
    "                            value_index = span_index - (indices_considered - current_index)\n",
    "                            doc_id = data_for_metrics['doc_id'][i]\n",
    "                            hypothesis_id = data_for_metrics['hypothesis_id'][i]\n",
    "                            span_id = data_for_metrics['span_ids'][i][value_index]\n",
    "                            key = str(doc_id)+ '-' + str(hypothesis_id)+ '-' + str(span_id)\n",
    "                            true_labels_per_span[key] = span_labels[span_index]\n",
    "                            if key in probs_per_span:\n",
    "                                probs_per_span[key].append(torch.sigmoid(span_logits[span_index]))\n",
    "                                # probs_per_span[key].append(span_logits[value_index])\n",
    "                            else:\n",
    "                                probs_per_span[key] = [torch.sigmoid(span_logits[span_index])]\n",
    "                                # probs_per_span[key] = [span_logits[value_index]]\n",
    "                        else: \n",
    "                            break \n",
    "                    \n",
    "                    span_indices_to_consider = span_indices_to_consider[cnt:]\n",
    "\n",
    "                # eval_span_preds = torch.tensor(eval_span_preds.squeeze(1), dtype=torch.long)\n",
    "\n",
    "                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n",
    "                eval_nli_labels.extend(nli_labels.cpu().numpy())\n",
    "                eval_nli_preds.extend(nli_preds.cpu().numpy())\n",
    "\n",
    "        eval_span_labels = []\n",
    "        eval_span_preds = []\n",
    "\n",
    "        for key in true_labels_per_span:\n",
    "            eval_span_labels.append(true_labels_per_span[key].item())\n",
    "            eval_span_preds.append(torch.mean(torch.stack(probs_per_span[key])).item())\n",
    "\n",
    "        ##### For NLI probablities #####\n",
    "\n",
    "        # for key in nli_metrics:\n",
    "        #     nli_metrics[key]['nli_logits'] = torch.stack(nli_metrics[key]['nli_logits'])\n",
    "        #     nli_metrics[key]['spans_contribution'] = torch.stack(nli_metrics[key]['spans_contribution'])\n",
    "\n",
    "        #     span_sum = torch.sum(nli_metrics[key]['spans_contribution'])\n",
    "        #     spans_contribution = nli_metrics[key]['spans_contribution'].transpose(0, -1) @ nli_metrics[key]['nli_logits']\n",
    "\n",
    "        #     eval_nli_preds.append(torch.argmax(torch.softmax(spans_contribution/span_sum, dim=0)).item())\n",
    "        #     eval_nli_labels.append(nli_metrics[key]['true_nli_labels'].item())\n",
    "\n",
    "        ##### END #####\n",
    "\n",
    "        eval_nli_acc = accuracy_score(eval_nli_labels, eval_nli_preds)\n",
    "\n",
    "        ic.enable()\n",
    "        \n",
    "        ic(list(zip(eval_span_labels, eval_span_preds)))\n",
    "        # ic(len(eval_span_labels), len(eval_span_preds))\n",
    "        # ic(sum(eval_span_labels), sum(eval_span_preds))\n",
    "\n",
    "        # find threshold for 80% recall\n",
    "        # precision, recall, thresholds = precision_recall_curve(eval_span_labels, eval_span_preds)\n",
    "\n",
    "\n",
    "        mAP = (average_precision_score(eval_span_labels, eval_span_preds, pos_label=0) + average_precision_score(eval_span_labels, eval_span_preds, pos_label=1))/2\n",
    "\n",
    "        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n",
    "        precision_at_80_recall = precision_at_recall(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds), 0.8)\n",
    "        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Entailment'])\n",
    "        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Contradiction'])\n",
    "        \n",
    "        return {\n",
    "            'mAP' : mAP,\n",
    "            'precision_at_80_recall' : precision_at_80_recall,\n",
    "            'nli_acc': eval_nli_acc,\n",
    "            'f1_score_for_entailment': f1_score_for_entailment,\n",
    "            'f1_score_for_contradiction': f1_score_for_contradiction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T14:19:55.534427Z",
     "iopub.status.busy": "2024-11-13T14:19:55.533730Z",
     "iopub.status.idle": "2024-11-13T14:19:55.547705Z",
     "shell.execute_reply": "2024-11-13T14:19:55.546545Z",
     "shell.execute_reply.started": "2024-11-13T14:19:55.534384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = ContractNLIMetricTrainer(\n",
    "    model=model,                      \n",
    "    args=training_args,               \n",
    "    # train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,      \n",
    "    data_collator=ContractNLIMetricTrainer.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "# ic.enable()\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mAP': 0.584293051888368,\n",
       " 'precision_at_80_recall': 0.3567567567567567,\n",
       " 'nli_acc': 0.6554621848739496,\n",
       " 'f1_score_for_entailment': 0.30112590299277603,\n",
       " 'f1_score_for_contradiction': 0.2663243589845487}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6051957,
     "sourceId": 9860908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6060849,
     "sourceId": 9872828,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6075717,
     "sourceId": 9892550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
