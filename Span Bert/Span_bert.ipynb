{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:37.934090Z",
     "iopub.status.busy": "2024-11-13T13:04:37.933255Z",
     "iopub.status.idle": "2024-11-13T13:04:42.711555Z",
     "shell.execute_reply": "2024-11-13T13:04:42.710724Z",
     "shell.execute_reply.started": "2024-11-13T13:04:37.934043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)\n",
    "import syslog\n",
    "import os\n",
    "import json\n",
    "from nltk import word_tokenize \n",
    "import re\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPUtils:\n",
    "    \n",
    "    LABELS = {\n",
    "        'NotMentioned': 0,\n",
    "        'Entailment': 1,\n",
    "        'Contradiction': 2,\n",
    "    }\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data(path: Union[str, Path]) -> Dict:\n",
    "        try:\n",
    "            path = Path(path)\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Data file not found at: {path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise json.JSONDecodeError(f\"Invalid JSON format in file: {path}\")\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        text = (text.replace('\\n', ' ')\n",
    "                   .strip()\n",
    "                   .lower())\n",
    "        \n",
    "        replacements = [\n",
    "            (r'\\\\t', ' '),\n",
    "            (r'\\\\r', ' '),\n",
    "            (r'(.)\\1{2,}', r'\\1'),\n",
    "            (r'\\s+', ' ')  \n",
    "        ]\n",
    "        \n",
    "        for pattern, replacement in replacements:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "            \n",
    "        return text.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_hypotheses(data: Dict) -> Dict[str, str]:\n",
    "\n",
    "        if not isinstance(data, dict) or 'labels' not in data:\n",
    "            raise ValueError(\"Invalid data format: missing 'labels' key\")\n",
    "            \n",
    "        return {\n",
    "            key: NLPUtils.clean_text(value.get('hypothesis', ''))\n",
    "            for key, value in data['labels'].items()\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hypothesis_index(hypothesis_name: str) -> Optional[int]:\n",
    "        try:\n",
    "            return int(hypothesis_name.split('-')[-1])\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> str:\n",
    "        try:\n",
    "            return ' '.join(word_tokenize(text))\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error: {e}\")\n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def get_labels(cls) -> Dict[str, int]:\n",
    "        return cls.LABELS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.766733Z",
     "iopub.status.busy": "2024-11-13T13:04:42.766251Z",
     "iopub.status.idle": "2024-11-13T13:04:42.775058Z",
     "shell.execute_reply": "2024-11-13T13:04:42.774126Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.766685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"batch_size\": 32,\n",
    "    \"train_path\": \"/kaggle/input/project-data/train (1).json\",\n",
    "    \"test_path\": \"/kaggle/input/project-data/test (1).json\",\n",
    "    \"dev_path\": \"/kaggle/input/project-data/dev (1).json\",\n",
    "    \"max_length\": 512,\n",
    "    \"models_save_dir\": \"/kaggle/working/saved_model\",\n",
    "    \"results_dir\": \"/kaggle/working/results\",\n",
    "    \"dataset_dir\": \"/kaggle/working/dataset_dir\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.776934Z",
     "iopub.status.busy": "2024-11-13T13:04:42.776554Z",
     "iopub.status.idle": "2024-11-13T13:04:42.785101Z",
     "shell.execute_reply": "2024-11-13T13:04:42.784307Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.776889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:42.786591Z",
     "iopub.status.busy": "2024-11-13T13:04:42.786288Z",
     "iopub.status.idle": "2024-11-13T13:04:44.018928Z",
     "shell.execute_reply": "2024-11-13T13:04:44.017975Z",
     "shell.execute_reply.started": "2024-11-13T13:04:42.786559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4777fcab0094906aa646ce36a66f518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d196416a418433289473755db5db40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9498fcdba93b4fa686420c1d232be064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe101b768f944debafd961280eae6fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/saved_model/tokenizer_config.json',\n",
       " '/kaggle/working/saved_model/special_tokens_map.json',\n",
       " '/kaggle/working/saved_model/vocab.txt',\n",
       " '/kaggle/working/saved_model/added_tokens.json',\n",
       " '/kaggle/working/saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "\n",
    "tokenizer.save_pretrained(cfg['models_save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:44.020734Z",
     "iopub.status.busy": "2024-11-13T13:04:44.020332Z",
     "iopub.status.idle": "2024-11-13T13:04:44.062312Z",
     "shell.execute_reply": "2024-11-13T13:04:44.061504Z",
     "shell.execute_reply.started": "2024-11-13T13:04:44.020689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['models_save_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:44.064348Z",
     "iopub.status.busy": "2024-11-13T13:04:44.063675Z",
     "iopub.status.idle": "2024-11-13T13:04:56.891407Z",
     "shell.execute_reply": "2024-11-13T13:04:56.890450Z",
     "shell.execute_reply.started": "2024-11-13T13:04:44.064300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icecream\n",
      "  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
      "Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: icecream\n",
      "Successfully installed icecream-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install icecream\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "\n",
    "    \"\"\"Represents a context window in the document.\"\"\"\n",
    "\n",
    "    doc_id: int\n",
    "    start_char_idx: int\n",
    "    end_char_idx: int\n",
    "    spans: List[Dict[str, Any]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataPoint:\n",
    "    \n",
    "    hypothesis: str\n",
    "    premise: str\n",
    "    marked_beg: bool\n",
    "    marked_end: bool\n",
    "    nli_label: torch.Tensor\n",
    "    span_labels: torch.Tensor\n",
    "    doc_id: torch.Tensor\n",
    "    hypothesis_id: torch.Tensor\n",
    "    span_ids: torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:56.902589Z",
     "iopub.status.busy": "2024-11-13T13:04:56.902215Z",
     "iopub.status.idle": "2024-11-13T13:04:56.928795Z",
     "shell.execute_reply": "2024-11-13T13:04:56.927912Z",
     "shell.execute_reply.started": "2024-11-13T13:04:56.902554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    \n",
    "    SPAN_TOKEN = '[SPAN]'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[Dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        hypothesis: Dict[str, str],\n",
    "        context_sizes: List[int],\n",
    "        surround_character_size: int\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': [self.SPAN_TOKEN]})\n",
    "        \n",
    "        contexts = self._generate_contexts(documents, context_sizes, surround_character_size)\n",
    "        self.data_points = self._create_data_points(documents, contexts, hypothesis)\n",
    "        self.span_token_id = self.tokenizer.convert_tokens_to_ids(self.SPAN_TOKEN)\n",
    "\n",
    "    def _generate_contexts(\n",
    "        self,\n",
    "        documents: List[Dict],\n",
    "        context_sizes: List[int],\n",
    "        surround_character_size: int\n",
    "    ) -> List[Context]:\n",
    "        \n",
    "        contexts = []\n",
    "        \n",
    "        for context_size in context_sizes:\n",
    "            for doc_id, doc in enumerate(documents):\n",
    "                char_idx = 0\n",
    "                document_spans = doc['spans']\n",
    "                \n",
    "                while char_idx < len(doc['text']):\n",
    "                    context = self._create_context(\n",
    "                        doc_id, char_idx, context_size, document_spans\n",
    "                    )\n",
    "                    \n",
    "                    if not contexts or context != contexts[-1]:\n",
    "                        contexts.append(context)\n",
    "                        \n",
    "                        # Update char_idx based on spans\n",
    "                        if (len(context.spans) == 1 and \n",
    "                            not context.spans[0]['marked']):\n",
    "                            char_idx = context.end_char_idx - surround_character_size\n",
    "                        else:\n",
    "                            char_idx = context.spans[-1]['start_char_idx'] - surround_character_size\n",
    "                    else:\n",
    "                        char_idx = context.end_char_idx - surround_character_size\n",
    "                        \n",
    "        return contexts\n",
    "\n",
    "    def _create_context(\n",
    "        self,\n",
    "        doc_id: int,\n",
    "        char_idx: int,\n",
    "        context_size: int,\n",
    "        document_spans: List[Tuple[int, int]]\n",
    "    ) -> Context:\n",
    "        \"\"\"Create a single context window.\"\"\"\n",
    "        context = Context(\n",
    "            doc_id=doc_id,\n",
    "            start_char_idx=char_idx,\n",
    "            end_char_idx=char_idx + context_size,\n",
    "            spans=[]\n",
    "        )\n",
    "        \n",
    "        for span_id, (start, end) in enumerate(document_spans):\n",
    "            if end <= char_idx:\n",
    "                continue\n",
    "                \n",
    "            context.spans.append({\n",
    "                'start_char_idx': max(start, char_idx),\n",
    "                'end_char_idx': min(end, char_idx + context_size),\n",
    "                'marked': start >= char_idx and end <= char_idx + context_size,\n",
    "                'span_id': span_id\n",
    "            })\n",
    "            \n",
    "            if end > char_idx + context_size:\n",
    "                break\n",
    "                \n",
    "        return context\n",
    "\n",
    "    def _create_data_points(\n",
    "        self,\n",
    "        documents: List[Dict],\n",
    "        contexts: List[Context],\n",
    "        hypothesis: Dict[str, str]\n",
    "    ) -> List[DataPoint]:\n",
    "        \n",
    "        data_points = []\n",
    "        label_dict = NLPUtils.get_labels() \n",
    "        \n",
    "        for nda_name, nda_desc in hypothesis.items():\n",
    "            for context in contexts:\n",
    "                doc = documents[context.doc_id]\n",
    "                nli_label = label_dict[doc['annotation_sets'][0]['annotations'][nda_name]['choice']]\n",
    "                \n",
    "                premise, span_ids, span_labels = self._process_spans(\n",
    "                    context, doc, nda_name\n",
    "                )\n",
    "                \n",
    "                if nli_label == label_dict['NotMentioned']:\n",
    "                    span_labels = torch.zeros(len(span_labels), dtype=torch.long)\n",
    "                else:\n",
    "                    span_labels = torch.tensor(span_labels, dtype=torch.long)\n",
    "                \n",
    "                data_point = DataPoint(\n",
    "                    hypothesis=nda_desc,\n",
    "                    premise=premise,\n",
    "                    marked_beg=context.spans[0]['marked'],\n",
    "                    marked_end=context.spans[-1]['marked'] or len(context.spans) == 1,\n",
    "                    nli_label=torch.tensor(nli_label, dtype=torch.long),\n",
    "                    span_labels=span_labels,\n",
    "                    doc_id=torch.tensor(context.doc_id, dtype=torch.long),\n",
    "                    hypothesis_id=torch.tensor(self._get_hypothesis_idx(nda_name), dtype=torch.long),\n",
    "                    span_ids=torch.tensor(span_ids, dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "                data_points.append(data_point)\n",
    "                \n",
    "        return data_points\n",
    "\n",
    "    def _process_spans(\n",
    "        self,\n",
    "        context: Context,\n",
    "        document: Dict,\n",
    "        nda_name: str\n",
    "    ) -> Tuple[str, List[int], List[int]]:\n",
    "        \n",
    "        premise = \"\"\n",
    "        span_ids = []\n",
    "        span_labels = []\n",
    "        \n",
    "        for span in context.spans:\n",
    "            # Calculate span label\n",
    "            is_relevant = int(span['span_id'] in \n",
    "                            document['annotation_sets'][0]['annotations'][nda_name]['spans'])\n",
    "            span_label = 2 * is_relevant - 1  # Convert 0->-1 and 1->1\n",
    "            \n",
    "            if span['marked']:\n",
    "                span_labels.append(span_label)\n",
    "                span_ids.append(span['span_id'])\n",
    "            \n",
    "            premise += f\" {self.SPAN_TOKEN} \"\n",
    "            premise += document['text'][span['start_char_idx']:span['end_char_idx']]\n",
    "            \n",
    "        return premise.strip(), span_ids, span_labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        data_point = self.data_points[idx]\n",
    "        \n",
    "        tokenized_data = self._tokenize_inputs(data_point)\n",
    "        \n",
    "        span_indices = self._process_span_indices(\n",
    "            tokenized_data, data_point.marked_beg, data_point.marked_end\n",
    "        )\n",
    "        \n",
    "        span_ids = data_point.span_ids[:len(span_indices)]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'],\n",
    "            'attention_mask': tokenized_data['attention_mask'],\n",
    "            'token_type_ids': tokenized_data['token_type_ids'],\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': data_point.nli_label,\n",
    "            'span_labels': data_point.span_labels[:len(span_indices)],\n",
    "            'data_for_metrics': {\n",
    "                'doc_id': data_point.doc_id,\n",
    "                'hypothesis_id': data_point.hypothesis_id,\n",
    "                'span_ids': span_ids,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _tokenize_inputs(self, data_point: DataPoint) -> Dict[str, torch.Tensor]:\n",
    "        tokenized = self.tokenizer(\n",
    "            [data_point.hypothesis],\n",
    "            [data_point.premise],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(),\n",
    "            'token_type_ids': tokenized['token_type_ids'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def _process_span_indices(\n",
    "        self,\n",
    "        tokenized_data: Dict[str, torch.Tensor],\n",
    "        marked_beg: bool,\n",
    "        marked_end: bool\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n",
    "        \n",
    "        if not marked_beg:\n",
    "            span_indices = span_indices[1:]\n",
    "            \n",
    "        if not marked_end or tokenized_data['attention_mask'][-1] == 0:\n",
    "            span_indices = span_indices[:-1]\n",
    "            \n",
    "        return span_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_hypothesis_idx(hypothesis_name: str) -> int:\n",
    "        return int(hypothesis_name.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \n",
    "    train_path: str\n",
    "    dev_path: str\n",
    "    test_path: str\n",
    "    context_size: int = 1100\n",
    "    surround_character_size: int = 50\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:04:58.643818Z",
     "iopub.status.busy": "2024-11-13T13:04:58.643022Z",
     "iopub.status.idle": "2024-11-13T13:05:07.463063Z",
     "shell.execute_reply": "2024-11-13T13:05:07.462257Z",
     "shell.execute_reply.started": "2024-11-13T13:04:58.643771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/1745476268.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "class NLIDataManager:\n",
    "    \"\"\"\n",
    "    Manages the loading, preprocessing, and creation of NLI datasets.\n",
    "    \n",
    "    Attributes:\n",
    "        config: DataConfig object containing configuration parameters\n",
    "        tokenizer: Tokenizer for processing text\n",
    "        hypothesis: Dictionary of hypotheses extracted from training data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DataConfig,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    "    ):\n",
    "        \n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hypothesis = None\n",
    "        self.datasets = {}\n",
    "        self._validate_paths()\n",
    "        \n",
    "    def _validate_paths(self) -> None:\n",
    "        \n",
    "        for path_name, path in {\n",
    "            'train': self.config.train_path,\n",
    "            'dev': self.config.dev_path,\n",
    "            'test': self.config.test_path\n",
    "        }.items():\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"{path_name} data path does not exist: {path}\"\n",
    "                )\n",
    "    \n",
    "    def load_and_preprocess(self) -> None:\n",
    "\n",
    "        logger.info(\"Loading and preprocessing datasets...\")\n",
    "        \n",
    "        train_data = self._load_data(self.config.train_path)\n",
    "        dev_data = self._load_data(self.config.dev_path)\n",
    "        test_data = self._load_data(self.config.test_path)\n",
    "        \n",
    "        self.hypothesis = self._extract_hypothesis(train_data)\n",
    "        \n",
    "        train_documents = train_data['documents']\n",
    "        dev_documents = dev_data['documents']\n",
    "        test_documents = test_data['documents']\n",
    "        \n",
    "        logger.info(f\"Loaded documents - Train: {len(train_documents)}, \"\n",
    "                   f\"Dev: {len(dev_documents)}, Test: {len(test_documents)}\")\n",
    "        \n",
    "        self.datasets = {\n",
    "            'train': self._create_dataset(train_documents, 'train'),\n",
    "            'dev': self._create_dataset(dev_documents, 'dev'),\n",
    "            'test': self._create_dataset(test_documents, 'test')\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Dataset creation completed successfully\")\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _load_data(path: str) -> Dict:\n",
    "\n",
    "        try:\n",
    "            return NLPUtils.load_data(path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data from {path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_hypothesis(data: Dict) -> Dict[str, str]:\n",
    "        try:\n",
    "            return NLPUtils.extract_hypotheses(data) \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting hypothesis: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_dataset(\n",
    "        self,\n",
    "        documents: List[Dict],\n",
    "        split: str\n",
    "    ) -> NLIDataset:\n",
    "\n",
    "        try:\n",
    "            return NLIDataset(\n",
    "                documents=documents,\n",
    "                tokenizer=self.tokenizer,\n",
    "                hypothesis=self.hypothesis,\n",
    "                context_sizes=[self.config.context_size],\n",
    "                surround_character_size=self.config.surround_character_size\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating {split} dataset: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_dataloaders(\n",
    "        self,\n",
    "        shuffle_train: bool = True\n",
    "    ) -> Dict[str, DataLoader]:\n",
    "\n",
    "        if not self.datasets:\n",
    "            raise ValueError(\"Datasets not initialized. Call load_and_preprocess first.\")\n",
    "        \n",
    "        return {\n",
    "            split: DataLoader(\n",
    "                dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=(shuffle_train and split == 'train'),\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            for split, dataset in self.datasets.items()\n",
    "        }\n",
    "    \n",
    "    def get_dataset(self, split: str) -> Optional[NLIDataset]:\n",
    "        return self.datasets.get(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DataConfig(\n",
    "    train_path=cfg['train_path'],\n",
    "    dev_path=cfg['dev_path'],\n",
    "    test_path=cfg['test_path']\n",
    ")\n",
    "\n",
    "data_manager = NLIDataManager(config, tokenizer)\n",
    "\n",
    "data_manager.load_and_preprocess()\n",
    "\n",
    "dataloaders = data_manager.get_dataloaders()\n",
    "\n",
    "train_dataset = data_manager.get_dataset('train')\n",
    "dev_dataset = data_manager.get_dataset('dev')\n",
    "test_dataset = data_manager.get_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:05:07.464417Z",
     "iopub.status.busy": "2024-11-13T13:05:07.464119Z",
     "iopub.status.idle": "2024-11-13T13:05:07.733155Z",
     "shell.execute_reply": "2024-11-13T13:05:07.732257Z",
     "shell.execute_reply.started": "2024-11-13T13:05:07.464386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(train_dataset): 97546\n",
      "    len(dev_dataset): 15385\n",
      "    len(test_dataset): 28645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97546, 15385, 28645)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(len(train_dataset), len(dev_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class WeightResult:\n",
    "    nli_weights: np.ndarray\n",
    "    span_weight: float\n",
    "    nli_class_distribution: dict\n",
    "    span_class_distribution: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassWeightCalculator:\n",
    "    \n",
    "    def __init__(self, exclude_span_label: int = -1):\n",
    "\n",
    "        self.exclude_span_label = exclude_span_label\n",
    "    \n",
    "    def calculate_weights(\n",
    "        self,\n",
    "        dataset: Union[Dataset, List[dict]]\n",
    "    ) -> WeightResult:\n",
    "\n",
    "        try:\n",
    "            nli_labels = self._extract_nli_labels(dataset)\n",
    "            span_labels = self._extract_span_labels(dataset)\n",
    "            \n",
    "            if not nli_labels or not span_labels:\n",
    "                raise ValueError(\"No labels found in dataset\")\n",
    "            \n",
    "            nli_weights = self._compute_nli_weights(nli_labels)\n",
    "            span_weight = self._compute_span_weight(span_labels)\n",
    "            \n",
    "            nli_distribution = self._compute_class_distribution(nli_labels)\n",
    "            span_distribution = self._compute_class_distribution(span_labels)\n",
    "            \n",
    "            return WeightResult(\n",
    "                nli_weights=nli_weights,\n",
    "                span_weight=span_weight,\n",
    "                nli_class_distribution=nli_distribution,\n",
    "                span_class_distribution=span_distribution\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating weights: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_nli_labels(self, dataset: Union[Dataset, List[dict]]) -> np.ndarray:\n",
    "        try:\n",
    "            if isinstance(dataset, Dataset):\n",
    "                labels = [dataset[i]['nli_label'] for i in range(len(dataset))]\n",
    "            else:\n",
    "                labels = [x['nli_label'] for x in dataset]\n",
    "            \n",
    "            labels = [\n",
    "                label.item() if isinstance(label, torch.Tensor) else label\n",
    "                for label in labels\n",
    "            ]\n",
    "            \n",
    "            return np.array(labels)\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error extracting NLI labels: {str(e)}\")\n",
    "    \n",
    "    def _extract_span_labels(self, dataset: Union[Dataset, List[dict]]) -> np.ndarray:\n",
    "        try:\n",
    "            span_labels = []\n",
    "            \n",
    "            if isinstance(dataset, Dataset):\n",
    "                for i in range(len(dataset)):\n",
    "                    labels = dataset[i]['span_labels']\n",
    "                    if isinstance(labels, torch.Tensor):\n",
    "                        labels = labels.numpy()\n",
    "                    span_labels.extend(labels)\n",
    "            else:\n",
    "                for item in dataset:\n",
    "                    labels = item['span_labels']\n",
    "                    if isinstance(labels, torch.Tensor):\n",
    "                        labels = labels.numpy()\n",
    "                    span_labels.extend(labels)\n",
    "            \n",
    "            span_labels = [\n",
    "                label for label in span_labels \n",
    "                if label != self.exclude_span_label\n",
    "            ]\n",
    "            \n",
    "            return np.array(span_labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error extracting span labels: {str(e)}\")\n",
    "    \n",
    "    def _compute_nli_weights(self, labels: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        unique_classes = np.unique(labels)\n",
    "        weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=unique_classes,\n",
    "            y=labels\n",
    "        )\n",
    "        return weights\n",
    "    \n",
    "    def _compute_span_weight(self, labels: np.ndarray) -> float:\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            raise ValueError(\"No valid span labels found\")\n",
    "            \n",
    "        n_negative = np.sum(labels == 0)\n",
    "        n_positive = np.sum(labels == 1)\n",
    "        \n",
    "        if n_positive == 0:\n",
    "            logger.warning(\"No positive span labels found\")\n",
    "            return 1.0\n",
    "            \n",
    "        return n_negative / n_positive\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_class_distribution(labels: np.ndarray) -> dict:\n",
    "    \n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        total = len(labels)\n",
    "        return {\n",
    "            label: {\n",
    "                'count': count,\n",
    "                'percentage': (count / total) * 100\n",
    "            }\n",
    "            for label, count in zip(unique, counts)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight_statistics(result: WeightResult) -> None:\n",
    "    \n",
    "    print(\"\\nClass Weight Statistics:\")\n",
    "    print(\"\\nNLI Classification:\")\n",
    "    \n",
    "    for class_idx, weight in enumerate(result.nli_weights):\n",
    "        dist = result.nli_class_distribution.get(class_idx, {})\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Weight: {weight:.3f}\")\n",
    "        print(f\"  Count: {dist.get('count', 0)}\")\n",
    "        print(f\"  Percentage: {dist.get('percentage', 0):.2f}%\")\n",
    "    \n",
    "    print(\"\\nSpan Classification:\")\n",
    "    print(f\"Positive/Negative Weight: {result.span_weight:.3f}\")\n",
    "    \n",
    "    for class_idx, stats in result.span_class_distribution.items():\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Count: {stats['count']}\")\n",
    "        print(f\"  Percentage: {stats['percentage']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:05:07.743369Z",
     "iopub.status.busy": "2024-11-13T13:05:07.743057Z",
     "iopub.status.idle": "2024-11-13T13:10:33.657826Z",
     "shell.execute_reply": "2024-11-13T13:10:33.657000Z",
     "shell.execute_reply.started": "2024-11-13T13:05:07.743337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "calculator = ClassWeightCalculator(exclude_span_label=-1)\n",
    "\n",
    "weights = calculator.calculate_weights(train_dataset)\n",
    "\n",
    "print_weight_statistics(weights)\n",
    "\n",
    "nli_weights = torch.tensor(weights.nli_weights, device=NLPUtils.DEVICE)\n",
    "span_weight = torch.tensor(weights.span_weight, device=NLPUtils.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:33.659279Z",
     "iopub.status.busy": "2024-11-13T13:10:33.658958Z",
     "iopub.status.idle": "2024-11-13T13:10:33.707815Z",
     "shell.execute_reply": "2024-11-13T13:10:33.706904Z",
     "shell.execute_reply.started": "2024-11-13T13:10:33.659245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| nli_weights: [0.9712447975785092, 0.6134852801519468, 2.9380440348182284]\n",
      "    span_weight: 24.93889485618809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9712447975785092, 0.6134852801519468, 2.9380440348182284],\n",
       " 24.93889485618809)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(nli_weights, span_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:33.709739Z",
     "iopub.status.busy": "2024-11-13T13:10:33.709254Z",
     "iopub.status.idle": "2024-11-13T13:10:34.487045Z",
     "shell.execute_reply": "2024-11-13T13:10:34.486254Z",
     "shell.execute_reply.started": "2024-11-13T13:10:33.709685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class ContractNLIConfig(PretrainedConfig):\n",
    "    \n",
    "    def __init__(self, nli_weights = [1, 1, 1], span_weight = 1, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(NLPUtils.get_labels()), ignore_span_label = 2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.lambda_ = lambda_\n",
    "        self.ignore_span_label = ignore_span_label\n",
    "        self.nli_weights = nli_weights\n",
    "        self.span_weight = span_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:34.488739Z",
     "iopub.status.busy": "2024-11-13T13:10:34.488287Z",
     "iopub.status.idle": "2024-11-13T13:10:34.506379Z",
     "shell.execute_reply": "2024-11-13T13:10:34.505338Z",
     "shell.execute_reply.started": "2024-11-13T13:10:34.488699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "class ContractNLI(PreTrainedModel):\n",
    "    config_class = ContractNLIConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n",
    "        self.bert.resize_token_embeddings(self.bert.config.vocab_size + 1, pad_to_multiple_of=8)\n",
    "        self.bert.eval()\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding_dim = self.bert.config.hidden_size\n",
    "        self.num_labels = config.num_labels\n",
    "        self.lambda_ = config.lambda_\n",
    "        self.nli_criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.config.nli_weights, dtype=torch.float32))\n",
    "        self.span_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.config.span_weight, dtype=torch.float32))\n",
    "\n",
    "        self.span_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 2, 1)\n",
    "        )\n",
    "\n",
    "        self.nli_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 4, self.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim * 2, self.num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, span_indices):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True).hidden_states[-4:]\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        outputs = outputs.permute([1, 2, 0, 3])\n",
    "        outputs = outputs.reshape([outputs.shape[0], outputs.shape[1], -1])\n",
    "\n",
    "        gather = torch.gather(outputs, 1, span_indices.unsqueeze(2).expand(-1, -1, outputs.shape[-1]))\n",
    "\n",
    "        masked_gather = gather[span_indices != 0]\n",
    "        span_logits = self.span_classifier(masked_gather)\n",
    "        nli_logits = self.nli_classifier(outputs[:, 0, :])\n",
    "\n",
    "        return span_logits, nli_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:34.508453Z",
     "iopub.status.busy": "2024-11-13T13:10:34.507898Z",
     "iopub.status.idle": "2024-11-13T13:10:48.833347Z",
     "shell.execute_reply": "2024-11-13T13:10:48.832532Z",
     "shell.execute_reply.started": "2024-11-13T13:10:34.508405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class ContractNLITrainer(Trainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        span_label = inputs.pop('span_labels')\n",
    "        nli_label = inputs.pop('nli_label')\n",
    "        inputs.pop('data_for_metrics')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        span_logits, nli_logits = outputs[0], outputs[1]\n",
    "        \n",
    "        mask = span_label != -1\n",
    "        span_label = span_label[mask]\n",
    "        span_logits = span_logits[mask]\n",
    "        \n",
    "        span_label = span_label.float()\n",
    "        span_logits = span_logits.float()\n",
    "        \n",
    "        span_label = span_label.view(-1)\n",
    "        span_logits = span_logits.view(-1)        \n",
    "\n",
    "        # if len(true_span_labels) == 0 or len(pred_span_labels) != len(true_span_labels):\n",
    "        #     span_loss = torch.tensor(0, dtype=torch.float32, device=NLPUtils.DEVICE)\n",
    "        # else:\n",
    "        #     span_loss = self.model.span_criterion(pred_span_labels, true_span_labels)\n",
    "        \n",
    "        if len(span_label) == 0:\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=NLPUtils.DEVICE)\n",
    "        else:\n",
    "            span_loss = self.model.span_criterion(span_logits, span_label)\n",
    "\n",
    "        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n",
    "\n",
    "        if torch.isnan(nli_loss):\n",
    "            nli_loss = torch.tensor(0, dtype=torch.float32, device=NLPUtils.DEVICE)\n",
    "\n",
    "        if torch.isnan(span_loss):\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=NLPUtils.DEVICE)\n",
    "\n",
    "        loss = span_loss + self.model.lambda_ * nli_loss\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            loss = torch.tensor(0, dtype=torch.float32, device=NLPUtils.DEVICE, requires_grad=True)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(features):\n",
    "        span_indices_list = [feature['span_indices'] for feature in features]\n",
    "        max_len = max([len(span_indices) for span_indices in span_indices_list])\n",
    "        span_indices_list = [torch.cat([span_indices, torch.zeros(max_len - len(span_indices), dtype=torch.long)]) for span_indices in span_indices_list]\n",
    "\n",
    "        span_ids_list = [feature['data_for_metrics']['span_ids'] for feature in features]\n",
    "        max_len = max([len(span_ids) for span_ids in span_ids_list])\n",
    "        \n",
    "        # pad to get the doc id and hypothesis id for each input while evaluating\n",
    "        span_ids_list = [torch.cat([span_ids, torch.full((max_len - len(span_ids),), -1)]) for span_ids in span_ids_list]\n",
    "        \n",
    "        input_ids = torch.stack([feature['input_ids'] for feature in features])\n",
    "        attention_mask = torch.stack([feature['attention_mask'] for feature in features])\n",
    "        token_type_ids = torch.stack([feature['token_type_ids'] for feature in features])\n",
    "        span_indices = torch.stack(span_indices_list)\n",
    "        nli_label = torch.stack([feature['nli_label'] for feature in features])\n",
    "        span_label = torch.cat([feature['span_labels'] for feature in features], dim=0)\n",
    "        data_for_metrics = {\n",
    "            'doc_id': torch.stack([feature['data_for_metrics']['doc_id'] for feature in features]),\n",
    "            'hypothesis_id': torch.stack([feature['data_for_metrics']['hypothesis_id'] for feature in features]),\n",
    "            'span_ids': torch.stack(span_ids_list),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': nli_label,\n",
    "            'span_labels': span_label,\n",
    "            'data_for_metrics': data_for_metrics,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.835005Z",
     "iopub.status.busy": "2024-11-13T13:10:48.834427Z",
     "iopub.status.idle": "2024-11-13T13:10:48.895472Z",
     "shell.execute_reply": "2024-11-13T13:10:48.894416Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.834972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],   \n",
    "    num_train_epochs=10,           \n",
    "    gradient_accumulation_steps=4,   \n",
    "    logging_strategy='epoch',\n",
    "    eval_steps=2,\n",
    "    save_steps=2,\n",
    "    logging_steps=2,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.896893Z",
     "iopub.status.busy": "2024-11-13T13:10:48.896588Z",
     "iopub.status.idle": "2024-11-13T13:10:48.902167Z",
     "shell.execute_reply": "2024-11-13T13:10:48.901279Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.896860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wandb_hp_space(trial):\n",
    "    return {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\n",
    "            \"name\": \"eval/loss\",\n",
    "            \"goal\": \"minimize\"\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\n",
    "                \"values\": [1e-5, 3e-5, 5e-5]\n",
    "            },\n",
    "            \"lambda_\": {\n",
    "                \"values\": [0.05, 0.1, 0.4]\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.908677Z",
     "iopub.status.busy": "2024-11-13T13:10:48.908371Z",
     "iopub.status.idle": "2024-11-13T13:10:48.914556Z",
     "shell.execute_reply": "2024-11-13T13:10:48.913652Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.908644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    if trial is None:\n",
    "        return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight))\n",
    "\n",
    "    return ContractNLI(ContractNLIConfig(nli_weights=nli_weights, span_weight=span_weight, lambda_=trial['lambda_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:10:48.915819Z",
     "iopub.status.busy": "2024-11-13T13:10:48.915540Z",
     "iopub.status.idle": "2024-11-13T13:10:52.727740Z",
     "shell.execute_reply": "2024-11-13T13:10:52.726696Z",
     "shell.execute_reply.started": "2024-11-13T13:10:48.915787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f03a6e54aa40739f71604b6e04fb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer = ContractNLITrainer(\n",
    "    model=None,                       \n",
    "    args=training_args,              \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=dev_dataset,     \n",
    "    data_collator=ContractNLITrainer.collate_fn,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n",
    "    model_init=model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T09:43:24.421175Z",
     "iopub.status.busy": "2024-11-11T09:43:24.420760Z",
     "iopub.status.idle": "2024-11-11T11:41:08.348319Z",
     "shell.execute_reply": "2024-11-11T11:41:08.347406Z",
     "shell.execute_reply.started": "2024-11-11T09:43:24.421128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7456' max='12420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7456/12420 1:57:38 < 1:18:20, 1.06 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.634800</td>\n",
       "      <td>3.922571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.283600</td>\n",
       "      <td>4.243315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.186900</td>\n",
       "      <td>2.967618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.090300</td>\n",
       "      <td>5.793064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>5.055171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7456, training_loss=2.2392635099877616, metrics={'train_runtime': 7060.2097, 'train_samples_per_second': 56.32, 'train_steps_per_second': 1.759, 'total_flos': 8.353250792946893e+16, 'train_loss': 2.2392635099877616, 'epoch': 5.9995976664655})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:23.766598Z",
     "iopub.status.busy": "2024-11-13T13:11:23.765748Z",
     "iopub.status.idle": "2024-11-13T13:11:23.771173Z",
     "shell.execute_reply": "2024-11-13T13:11:23.770140Z",
     "shell.execute_reply.started": "2024-11-13T13:11:23.766557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:23.989070Z",
     "iopub.status.busy": "2024-11-13T13:11:23.988365Z",
     "iopub.status.idle": "2024-11-13T13:11:23.993842Z",
     "shell.execute_reply": "2024-11-13T13:11:23.992931Z",
     "shell.execute_reply.started": "2024-11-13T13:11:23.989030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['WANDB_ENTITY'] = 'contract-nli-db'\n",
    "os.environ['WANDB_PROJECT'] = 'contract-nli-metric'\n",
    "os.environ['WANDB_LOG_MODEL'] = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.397342Z",
     "iopub.status.busy": "2024-11-13T13:11:24.396572Z",
     "iopub.status.idle": "2024-11-13T13:11:24.405080Z",
     "shell.execute_reply": "2024-11-13T13:11:24.404122Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.397302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_path': '/kaggle/input/project-data/train (1).json',\n",
       " 'test_path': '/kaggle/input/project-data/test (1).json',\n",
       " 'dev_path': '/kaggle/input/project-data/dev (1).json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': '/kaggle/input/anlp-project-trained-model/checkpoint',\n",
       " 'dataset_dir': './scratch/shu7bh/contract_nli/dataset',\n",
       " 'results_dir': './scratch/shu7bh/contract_nli/results',\n",
       " 'trained_model_dir': '/kaggle/input/anlp-project-trained-model/',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cfg = {\n",
    "    \"train_path\": \"/kaggle/input/project-data/train (1).json\",\n",
    "    \"test_path\": \"/kaggle/input/project-data/test (1).json\",\n",
    "    \"dev_path\": \"/kaggle/input/project-data/dev (1).json\",\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"max_length\": 512,\n",
    "    \"models_save_dir\": \"/kaggle/input/anlp-project-trained-model/checkpoint\",\n",
    "    \"dataset_dir\": \"./scratch/shu7bh/contract_nli/dataset\",\n",
    "    \"results_dir\": \"./scratch/shu7bh/contract_nli/results\",\n",
    "    \"trained_model_dir\": \"/kaggle/input/anlp-project-trained-model/\",\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.589322Z",
     "iopub.status.busy": "2024-11-13T13:11:24.588588Z",
     "iopub.status.idle": "2024-11-13T13:11:24.603685Z",
     "shell.execute_reply": "2024-11-13T13:11:24.602745Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.589281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:24.820176Z",
     "iopub.status.busy": "2024-11-13T13:11:24.819300Z",
     "iopub.status.idle": "2024-11-13T13:11:24.987468Z",
     "shell.execute_reply": "2024-11-13T13:11:24.986482Z",
     "shell.execute_reply.started": "2024-11-13T13:11:24.820132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:11:25.040608Z",
     "iopub.status.busy": "2024-11-13T13:11:25.039896Z",
     "iopub.status.idle": "2024-11-13T13:11:36.679615Z",
     "shell.execute_reply": "2024-11-13T13:11:36.678400Z",
     "shell.execute_reply.started": "2024-11-13T13:11:25.040562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: icecream in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from icecream) (0.4.6)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.18.0)\n",
      "Requirement already satisfied: executing>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from icecream) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install icecream\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:15:59.572050Z",
     "iopub.status.busy": "2024-11-13T13:15:59.571256Z",
     "iopub.status.idle": "2024-11-13T13:16:01.973821Z",
     "shell.execute_reply": "2024-11-13T13:16:01.972781Z",
     "shell.execute_reply.started": "2024-11-13T13:15:59.572010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/1745476268.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "dev_data = load_data(os.path.join(cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(dev_data)\n",
    "\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "# dev_data = dev_data[:50]\n",
    "# test_data = test_data[:50]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(dev_data), len(test_data))\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1100], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1100], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.975854Z",
     "iopub.status.busy": "2024-11-13T13:16:01.975540Z",
     "iopub.status.idle": "2024-11-13T13:16:01.980990Z",
     "shell.execute_reply": "2024-11-13T13:16:01.979941Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.975820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15385\n",
      "28645\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.982488Z",
     "iopub.status.busy": "2024-11-13T13:16:01.982137Z",
     "iopub.status.idle": "2024-11-13T13:16:01.993820Z",
     "shell.execute_reply": "2024-11-13T13:16:01.992952Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.982454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "def get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return np.interp(recall_level, recall[::-1], precision[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:01.996042Z",
     "iopub.status.busy": "2024-11-13T13:16:01.995718Z",
     "iopub.status.idle": "2024-11-13T13:16:02.008766Z",
     "shell.execute_reply": "2024-11-13T13:16:02.007841Z",
     "shell.execute_reply.started": "2024-11-13T13:16:01.996008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import numpy and sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def calculate_micro_average_precision(y_true, y_pred):\n",
    "\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    if num_classes == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = 0.0\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        y_true_indices = np.where(y_true == class_idx)\n",
    "        average_precision += ic(precision_score(\n",
    "            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
    "        ))\n",
    "\n",
    "    return average_precision / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.010166Z",
     "iopub.status.busy": "2024-11-13T13:16:02.009879Z",
     "iopub.status.idle": "2024-11-13T13:16:02.023568Z",
     "shell.execute_reply": "2024-11-13T13:16:02.022770Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.010135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score_for_class(y_true, y_pred, class_idx):\n",
    "    \n",
    "    y_true_indices = np.where(y_true == class_idx)\n",
    "    \n",
    "    return f1_score(\n",
    "        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.024946Z",
     "iopub.status.busy": "2024-11-13T13:16:02.024629Z",
     "iopub.status.idle": "2024-11-13T13:16:02.034045Z",
     "shell.execute_reply": "2024-11-13T13:16:02.033189Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.024908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def precision_at_recall(y_true, y_scores, recall_threshold):\n",
    "    precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "    idx = (np.abs(recall - recall_threshold)).argmin() \n",
    "    ic(threshold[idx])\n",
    "    return precision[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.035536Z",
     "iopub.status.busy": "2024-11-13T13:16:02.035247Z",
     "iopub.status.idle": "2024-11-13T13:16:02.069079Z",
     "shell.execute_reply": "2024-11-13T13:16:02.068183Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.035505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],  \n",
    "    num_train_epochs=10,       \n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_strategy='epoch',\n",
    "    # eval_steps=0.25,\n",
    "    # save_steps=0.25,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.070545Z",
     "iopub.status.busy": "2024-11-13T13:16:02.070167Z",
     "iopub.status.idle": "2024-11-13T13:16:02.076385Z",
     "shell.execute_reply": "2024-11-13T13:16:02.075479Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.070500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/anlp-project-trained-model/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['trained_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T13:16:02.078298Z",
     "iopub.status.busy": "2024-11-13T13:16:02.077666Z",
     "iopub.status.idle": "2024-11-13T13:16:03.189546Z",
     "shell.execute_reply": "2024-11-13T13:16:03.188512Z",
     "shell.execute_reply.started": "2024-11-13T13:16:02.078253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "artifact_dir = '/kaggle/input/fully-trained-model/checkpoint-12194'  \n",
    "model = ContractNLI.from_pretrained(artifact_dir).to(NLPUtils.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T14:31:46.799835Z",
     "iopub.status.busy": "2024-11-13T14:31:46.799461Z",
     "iopub.status.idle": "2024-11-13T14:31:46.825459Z",
     "shell.execute_reply": "2024-11-13T14:31:46.824509Z",
     "shell.execute_reply.started": "2024-11-13T14:31:46.799795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContractNLIMetricTrainer(ContractNLITrainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        self.model.eval()\n",
    "        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n",
    "\n",
    "        eval_nli_labels = []\n",
    "        eval_nli_preds = []\n",
    "        true_labels_per_span = {}\n",
    "        probs_per_span = {}\n",
    "\n",
    "        nli_metrics = {}\n",
    "\n",
    "        for inputs in tqdm(self.dataloader):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            span_labels = inputs.pop('span_labels')\n",
    "            nli_labels = inputs.pop('nli_label')\n",
    "            data_for_metrics = inputs.pop('data_for_metrics')\n",
    "\n",
    "            span_indices_to_consider = torch.where(span_labels != -1)[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "                span_labels = span_labels.float()\n",
    "                span_logits = span_logits.float()\n",
    "                \n",
    "                span_labels = span_labels.view(-1)\n",
    "                span_logits = span_logits.view(-1)\n",
    "\n",
    "                # start_index = 0\n",
    "                \n",
    "                indices_considered = 0 # total number of span indices considered\n",
    "\n",
    "                # find the corresponding span index in data_for_metrics['span_ids'] considering -1 to be padding index\n",
    "                # ic(span_index)\n",
    "                for i, span_index_row in enumerate(data_for_metrics['span_ids']):\n",
    "                    current_index = 0 # current row's first -1 index\n",
    "                    # ic(span_index_row)\n",
    "                    first_minus_one_index = torch.where(span_index_row == -1)[0]\n",
    "                    # ic(first_minus_one_index)\n",
    "                    if len(first_minus_one_index) == 0:\n",
    "                        first_minus_one_index = len(span_index_row)\n",
    "                    else:\n",
    "                        first_minus_one_index = first_minus_one_index[0].item()\n",
    "\n",
    "                    key = str(data_for_metrics['doc_id'][i].item())+ '-' + str(data_for_metrics['hypothesis_id'][i].item())\n",
    "\n",
    "                    # mask span_labels and span_logits for the current row\n",
    "                    mask = span_labels[indices_considered:indices_considered+first_minus_one_index] != -1\n",
    "                    span_logits_masked = span_logits[indices_considered:indices_considered+first_minus_one_index][mask]\n",
    "\n",
    "                    spans_contribution = torch.sum(torch.sigmoid(span_logits_masked)) / (len(span_logits_masked)) \n",
    "\n",
    "                    if key in nli_metrics:\n",
    "                        nli_metrics[key]['spans_contribution'].append(spans_contribution)\n",
    "                        nli_metrics[key]['nli_logits'].append(nli_logits[i])\n",
    "                    else:\n",
    "                        nli_metrics[key] = {}\n",
    "                        nli_metrics[key]['true_nli_labels'] = nli_labels[i]\n",
    "                        nli_metrics[key]['spans_contribution'] = [spans_contribution]\n",
    "                        nli_metrics[key]['nli_logits'] = [nli_logits[i]]\n",
    "                    \n",
    "                    current_index = first_minus_one_index\n",
    "                    indices_considered += current_index\n",
    "                    \n",
    "                    # ic(indices_considered)\n",
    "                    # ic(current_index)\n",
    "                    cnt = 0 # count to keep track of the number of span indices added in dictionary\n",
    "                    \n",
    "                    for span_index in span_indices_to_consider:\n",
    "\n",
    "                        if span_index < indices_considered:\n",
    "                            cnt += 1\n",
    "                            value_index = span_index - (indices_considered - current_index)\n",
    "                            doc_id = data_for_metrics['doc_id'][i]\n",
    "                            hypothesis_id = data_for_metrics['hypothesis_id'][i]\n",
    "                            span_id = data_for_metrics['span_ids'][i][value_index]\n",
    "                            key = str(doc_id)+ '-' + str(hypothesis_id)+ '-' + str(span_id)\n",
    "                            true_labels_per_span[key] = span_labels[span_index]\n",
    "                            if key in probs_per_span:\n",
    "                                probs_per_span[key].append(torch.sigmoid(span_logits[span_index]))\n",
    "                                # probs_per_span[key].append(span_logits[value_index])\n",
    "                            else:\n",
    "                                probs_per_span[key] = [torch.sigmoid(span_logits[span_index])]\n",
    "                                # probs_per_span[key] = [span_logits[value_index]]\n",
    "                        else: \n",
    "                            break \n",
    "                    \n",
    "                    span_indices_to_consider = span_indices_to_consider[cnt:]\n",
    "\n",
    "                # eval_span_preds = torch.tensor(eval_span_preds.squeeze(1), dtype=torch.long)\n",
    "\n",
    "                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n",
    "                eval_nli_labels.extend(nli_labels.cpu().numpy())\n",
    "                eval_nli_preds.extend(nli_preds.cpu().numpy())\n",
    "\n",
    "        eval_span_labels = []\n",
    "        eval_span_preds = []\n",
    "\n",
    "        for key in true_labels_per_span:\n",
    "            eval_span_labels.append(true_labels_per_span[key].item())\n",
    "            eval_span_preds.append(torch.mean(torch.stack(probs_per_span[key])).item())\n",
    "\n",
    "        ##### For NLI probablities #####\n",
    "\n",
    "        # for key in nli_metrics:\n",
    "        #     nli_metrics[key]['nli_logits'] = torch.stack(nli_metrics[key]['nli_logits'])\n",
    "        #     nli_metrics[key]['spans_contribution'] = torch.stack(nli_metrics[key]['spans_contribution'])\n",
    "\n",
    "        #     span_sum = torch.sum(nli_metrics[key]['spans_contribution'])\n",
    "        #     spans_contribution = nli_metrics[key]['spans_contribution'].transpose(0, -1) @ nli_metrics[key]['nli_logits']\n",
    "\n",
    "        #     eval_nli_preds.append(torch.argmax(torch.softmax(spans_contribution/span_sum, dim=0)).item())\n",
    "        #     eval_nli_labels.append(nli_metrics[key]['true_nli_labels'].item())\n",
    "\n",
    "        ##### END #####\n",
    "\n",
    "        eval_nli_acc = accuracy_score(eval_nli_labels, eval_nli_preds)\n",
    "\n",
    "        ic.enable()\n",
    "        \n",
    "        ic(list(zip(eval_span_labels, eval_span_preds)))\n",
    "        # ic(len(eval_span_labels), len(eval_span_preds))\n",
    "        # ic(sum(eval_span_labels), sum(eval_span_preds))\n",
    "\n",
    "        # find threshold for 80% recall\n",
    "        # precision, recall, thresholds = precision_recall_curve(eval_span_labels, eval_span_preds)\n",
    "\n",
    "\n",
    "        mAP = (average_precision_score(eval_span_labels, eval_span_preds, pos_label=0) + average_precision_score(eval_span_labels, eval_span_preds, pos_label=1))/2\n",
    "\n",
    "        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n",
    "        precision_at_80_recall = precision_at_recall(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds), 0.8)\n",
    "        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), NLPUtils.get_labels()['Entailment'])\n",
    "        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), NLPUtils.get_labels()['Contradiction'])\n",
    "        \n",
    "        return {\n",
    "            'mAP' : mAP,\n",
    "            'precision_at_80_recall' : precision_at_80_recall,\n",
    "            'nli_acc': eval_nli_acc,\n",
    "            'f1_score_for_entailment': f1_score_for_entailment,\n",
    "            'f1_score_for_contradiction': f1_score_for_contradiction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T14:19:55.534427Z",
     "iopub.status.busy": "2024-11-13T14:19:55.533730Z",
     "iopub.status.idle": "2024-11-13T14:19:55.547705Z",
     "shell.execute_reply": "2024-11-13T14:19:55.546545Z",
     "shell.execute_reply.started": "2024-11-13T14:19:55.534384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = ContractNLIMetricTrainer(\n",
    "    model=model,                      \n",
    "    args=training_args,               \n",
    "    # train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,      \n",
    "    data_collator=ContractNLIMetricTrainer.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "# ic.enable()\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mAP': 0.584293051888368,\n",
       " 'precision_at_80_recall': 0.3567567567567567,\n",
       " 'nli_acc': 0.6554621848739496,\n",
       " 'f1_score_for_entailment': 0.30112590299277603,\n",
       " 'f1_score_for_contradiction': 0.2663243589845487}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6051957,
     "sourceId": 9860908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6060849,
     "sourceId": 9872828,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6075717,
     "sourceId": 9892550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
